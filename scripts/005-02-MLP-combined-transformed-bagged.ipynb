{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shift = 200\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(charcode):\n",
    "    r = 0\n",
    "    ln = len(str(charcode))\n",
    "    for i in range(ln):\n",
    "        r += (ord(str(charcode)[i]) - ord('A') + 1) * 26 ** (ln - i - 1)\n",
    "    return r\n",
    "\n",
    "fair_constant = 0.7\n",
    "def fair_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    x = (preds - labels)\n",
    "    den = abs(x) + fair_constant\n",
    "    grad = fair_constant * x / (den)\n",
    "    hess = fair_constant * fair_constant / (den * den)\n",
    "    return grad, hess\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)\n",
    "def mungeskewed(train, test, numeric_feats):\n",
    "    ntrain = train.shape[0]\n",
    "    test['loss'] = 0\n",
    "    train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "    skewed_feats = skewed_feats.index\n",
    "\n",
    "    for feats in skewed_feats:\n",
    "        train_test[feats] = train_test[feats] + 1\n",
    "        train_test[feats], lam = boxcox(train_test[feats])\n",
    "    return train_test, ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started\n",
      "\n",
      "('Combining Columns:', 'cat80_cat87')\n",
      "('Combining Columns:', 'cat80_cat57')\n",
      "('Combining Columns:', 'cat80_cat12')\n",
      "('Combining Columns:', 'cat80_cat79')\n",
      "('Combining Columns:', 'cat80_cat10')\n",
      "('Combining Columns:', 'cat80_cat7')\n",
      "('Combining Columns:', 'cat80_cat89')\n",
      "('Combining Columns:', 'cat80_cat2')\n",
      "('Combining Columns:', 'cat80_cat72')\n",
      "('Combining Columns:', 'cat80_cat81')\n",
      "('Combining Columns:', 'cat80_cat11')\n",
      "('Combining Columns:', 'cat80_cat1')\n",
      "('Combining Columns:', 'cat80_cat13')\n",
      "('Combining Columns:', 'cat80_cat9')\n",
      "('Combining Columns:', 'cat80_cat3')\n",
      "('Combining Columns:', 'cat80_cat16')\n",
      "('Combining Columns:', 'cat80_cat90')\n",
      "('Combining Columns:', 'cat80_cat23')\n",
      "('Combining Columns:', 'cat80_cat36')\n",
      "('Combining Columns:', 'cat80_cat73')\n",
      "('Combining Columns:', 'cat80_cat103')\n",
      "('Combining Columns:', 'cat80_cat40')\n",
      "('Combining Columns:', 'cat80_cat28')\n",
      "('Combining Columns:', 'cat80_cat111')\n",
      "('Combining Columns:', 'cat80_cat6')\n",
      "('Combining Columns:', 'cat80_cat76')\n",
      "('Combining Columns:', 'cat80_cat50')\n",
      "('Combining Columns:', 'cat80_cat5')\n",
      "('Combining Columns:', 'cat80_cat4')\n",
      "('Combining Columns:', 'cat80_cat14')\n",
      "('Combining Columns:', 'cat80_cat38')\n",
      "('Combining Columns:', 'cat80_cat24')\n",
      "('Combining Columns:', 'cat80_cat82')\n",
      "('Combining Columns:', 'cat80_cat25')\n",
      "('Combining Columns:', 'cat87_cat57')\n",
      "('Combining Columns:', 'cat87_cat12')\n",
      "('Combining Columns:', 'cat87_cat79')\n",
      "('Combining Columns:', 'cat87_cat10')\n",
      "('Combining Columns:', 'cat87_cat7')\n",
      "('Combining Columns:', 'cat87_cat89')\n",
      "('Combining Columns:', 'cat87_cat2')\n",
      "('Combining Columns:', 'cat87_cat72')\n",
      "('Combining Columns:', 'cat87_cat81')\n",
      "('Combining Columns:', 'cat87_cat11')\n",
      "('Combining Columns:', 'cat87_cat1')\n",
      "('Combining Columns:', 'cat87_cat13')\n",
      "('Combining Columns:', 'cat87_cat9')\n",
      "('Combining Columns:', 'cat87_cat3')\n",
      "('Combining Columns:', 'cat87_cat16')\n",
      "('Combining Columns:', 'cat87_cat90')\n",
      "('Combining Columns:', 'cat87_cat23')\n",
      "('Combining Columns:', 'cat87_cat36')\n",
      "('Combining Columns:', 'cat87_cat73')\n",
      "('Combining Columns:', 'cat87_cat103')\n",
      "('Combining Columns:', 'cat87_cat40')\n",
      "('Combining Columns:', 'cat87_cat28')\n",
      "('Combining Columns:', 'cat87_cat111')\n",
      "('Combining Columns:', 'cat87_cat6')\n",
      "('Combining Columns:', 'cat87_cat76')\n",
      "('Combining Columns:', 'cat87_cat50')\n"
     ]
    }
   ],
   "source": [
    "print('\\nStarted')\n",
    "directory = '../input/'\n",
    "train = pd.read_csv(directory + 'train.csv')\n",
    "#     train = train.sample(n=1000,random_state = 0,replace=True)\n",
    "test = pd.read_csv(directory + 'test.csv')\n",
    "\n",
    "numeric_feats = [x for x in train.columns[1:-1] if 'cont' in x]\n",
    "categorical_feats = [x for x in train.columns[1:-1] if 'cat' in x]\n",
    "train_test, ntrain = mungeskewed(train, test, numeric_feats)\n",
    "\n",
    "# taken from Vladimir's script (https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114)\n",
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = remove_train.union(remove_test)\n",
    "\n",
    "\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "\n",
    "        train_test[column] = train_test[column].apply(lambda x: filter_cat(x), 1)\n",
    "\n",
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "train_test[\"cont1\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont1\"]))\n",
    "train_test[\"cont4\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont4\"]))\n",
    "train_test[\"cont5\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont5\"]))\n",
    "train_test[\"cont8\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont8\"]))\n",
    "train_test[\"cont10\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont10\"]))\n",
    "train_test[\"cont11\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont11\"]))\n",
    "train_test[\"cont12\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont12\"]))\n",
    "\n",
    "train_test[\"cont6\"] = np.log(preprocessing.minmax_scale(train_test[\"cont6\"]) + 0000.1)\n",
    "train_test[\"cont7\"] = np.log(preprocessing.minmax_scale(train_test[\"cont7\"]) + 0000.1)\n",
    "train_test[\"cont9\"] = np.log(preprocessing.minmax_scale(train_test[\"cont9\"]) + 0000.1)\n",
    "train_test[\"cont13\"] = np.log(preprocessing.minmax_scale(train_test[\"cont13\"]) + 0000.1)\n",
    "train_test[\"cont14\"] = (np.maximum(train_test[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n",
    "\n",
    "print('')\n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    train_test[feat] = train_test[comb[0]] + train_test[comb[1]]\n",
    "    train_test[feat] = train_test[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n",
    "\n",
    "print('')\n",
    "for col in categorical_feats:\n",
    "    print('Analyzing Column:', col)\n",
    "    train_test[col] = train_test[col].apply(encode)\n",
    "\n",
    "print(train_test[categorical_feats])\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_test[numeric_feats] = \\\n",
    "    ss.fit_transform(train_test[numeric_feats].values)\n",
    "\n",
    "train = train_test.iloc[:ntrain, :].copy()\n",
    "test = train_test.iloc[ntrain:, :].copy()\n",
    "\n",
    "print('\\nMedian Loss:', train.loss.median())\n",
    "print('Mean Loss:', train.loss.mean())\n",
    "\n",
    "ids = pd.read_csv('../input/test.csv')['id']\n",
    "train_y = np.log(train['loss'] + shift)\n",
    "train_x = train.drop(['loss','id'], axis=1)\n",
    "test_x = test.drop(['loss','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "cv_sum = 0\n",
    "# early_stopping = 100\n",
    "fpred = []\n",
    "seed = 0\n",
    "\n",
    "kf = KFold(train.shape[0], n_folds=n_folds)\n",
    "pred_x_train = np.zeros((train.shape[0],1))\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d' % (i+1))\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "\n",
    "    mlpParamGrid = { 'hidden_layer_sizes':[\n",
    "                                           (800,2),\n",
    "                                           (900,2),\n",
    "                                           (1000,2),\n",
    "                                           (1200,2),\n",
    "                                           (1500,2),\n",
    "                                           (2500,2),\n",
    "                                           (3500,2),\n",
    "                                           (4500,2),\n",
    "                                           (5000,2),\n",
    "            \n",
    "                                           (800,3),\n",
    "                                           (900,3),\n",
    "                                           (1000,3),\n",
    "                                           (1200,3),\n",
    "                                           (1500,3),\n",
    "                                           (2500,3),\n",
    "                                           (3500,3),\n",
    "                                           (4500,3),\n",
    "                                           (5000,3),\n",
    "\n",
    "                                          ],\n",
    "                 'activation':['tanh'],\n",
    "               }\n",
    "    \n",
    "    clf = GridSearchCV(MLPRegressor(random_state=seed,early_stopping=True,max_iter=400,tol=0.0001), mlpParamGrid,                \n",
    "                   n_jobs=16, refit=True,cv=5, scoring = 'neg_mean_absolute_error')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    scores_val = clf.predict(X_val)\n",
    "    pred_x_train[test_index,0] = clf.predict(X_val)\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    print('eval-MAE: %.6f' % cv_score)\n",
    "    y_pred = np.exp(clf.predict(test_x)) - shift\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score\n",
    "\n",
    "mpred = pred / n_folds\n",
    "score = cv_sum / n_folds\n",
    "print('Average eval-MAE: %.6f' % score)\n",
    "\n",
    "print(\"Writing results\")\n",
    "result = pd.DataFrame(mpred, columns=['loss'])\n",
    "result[\"id\"] = ids\n",
    "result = result.set_index(\"id\")\n",
    "print(\"%d-fold average prediction:\" % n_folds)\n",
    "\n",
    "now = datetime.now()\n",
    "score = str(round((cv_sum / n_folds), 6))\n",
    "sub_file = '../output/mlp_combined_transformed_10folds.csv'\n",
    "print(\"Writing submission: %s\" % sub_file)\n",
    "result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(pred_x_train, '../output/mlp_combined_transformed_10folds_pred_on_trian.sav', compress=9)\n",
    "# joblib.dump(train_y, '../output/xgb_combined_transformed_10folds_trian_true_y.sav', compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
