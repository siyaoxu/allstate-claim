{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sp\n",
    "\n",
    "# from scipy.special import erfinv\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train_raw = pd.read_csv('../input/train.csv')\n",
    "# data_train_raw = data_train_raw.sample(frac=0.01, random_state=0)\n",
    "data_test_raw = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188318, 132)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ylog=np.log1p(data_train_raw['loss'])\n",
    "# ymean=ylog.mean()\n",
    "# ystd=ylog.std()\n",
    "ymean=ylog.min()\n",
    "ystd=ylog.max()\n",
    "data_train_raw['loss_g']=(ylog-ymean)/ystd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean outliers in training data\n",
    "lossMean = data_train_raw['loss_g'].mean()\n",
    "lossStd = data_train_raw['loss_g'].std()\n",
    "print('Mean of log1py: {}'.format(lossMean))\n",
    "print('Std of log1py: {}'.format(lossStd))\n",
    "lbound = lossMean-3.0*lossStd\n",
    "ubound = lossMean+3.0*lossStd\n",
    "print('Lower clipping bound: {}\\nUpper clipping bound: {}'.format(lbound, ubound))\n",
    "data_train_raw = data_train_raw[(data_train_raw['loss_g']>=lbound) & (data_train_raw['loss_g']<=ubound)]\n",
    "print('Shape of cleaned data: {}'.format(data_train_raw.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ToUniform(y):\n",
    "    z = norm.cdf(-y/np.sqrt(2))\n",
    "    return z\n",
    "def UniformToGauss(z):\n",
    "    return -np.sqrt(2)*norm.ppf(z)*ystd+ymean\n",
    "# def BackToOriginal(z):\n",
    "#     return np.exp(UniformToGauss(z))\n",
    "\n",
    "data_train_raw['loss_u']=ToUniform(data_train_raw['loss_g'])\n",
    "\n",
    "def lossRestore(logloss, ymean, ystd):\n",
    "    loss = np.expm1(logloss*ystd+ymean)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(data_train_raw['loss'],100)\n",
    "plt.title('loss');\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(ylog,100)\n",
    "plt.title('log(loss) - Gauss');\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(data_train_raw['loss_g'],100)\n",
    "plt.title('Normal');\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(lossRestore(data_train_raw['loss_g'],ymean,ystd),100)\n",
    "plt.title('Normal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation - labeling encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save label in a seperate serie\n",
    "labelSs = data_train_raw['loss_g'] \n",
    "trainDf = data_train_raw.drop(['id','loss','loss_g','loss_u'],axis=1)\n",
    "subId = data_test_raw['id']\n",
    "testDf = data_test_raw.drop(['id'],axis=1)\n",
    "\n",
    "print('trainDf has features from the raw data:\\n{}'.format(trainDf.columns))\n",
    "print('testDf has features from the raw data:\\n{}'.format(testDf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataAll = pd.concat([trainDf,testDf])\n",
    "dataCatAll = dataAll.select_dtypes(include=['object'])\n",
    "dataFltAll = dataAll.select_dtypes(include=['float64'])\n",
    "print('the stacked data\\'s dimension are:\\n{}'.format(dataAll.shape))\n",
    "print('{} of which are categorical'.format(dataCatAll.shape))\n",
    "print('{} of which are continuous'.format(dataFltAll.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataCatAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LevelList = {'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'}\n",
    "def LetterRep(x,LevelList):\n",
    "    if x in LevelList:\n",
    "        x = '0'+x\n",
    "    return x\n",
    "        \n",
    "dataCatAll = dataCatAll.applymap(lambda x: LetterRep(x,LevelList))\n",
    "dataCatAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = dataCatAll.columns[98:]\n",
    "dataCatAll_2 = dataCatAll[features]\n",
    "cats = [feature for feature in features if feature.startswith('cat')]\n",
    "for feat in cats:\n",
    "    dataCatAll_2[feat] = pd.factorize(dataCatAll_2[feat], sort=True)[0]\n",
    "dataCatAll_2.head()\n",
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataCatAll = dataCatAll.drop(features,axis=1)\n",
    "dataCatAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in dataCatAll.columns:\n",
    "    if (col.find('cat') !=-1):\n",
    "#        print(col)\n",
    "        dataCatAll[col]=le.fit_transform(dataCatAll[col])\n",
    "#         dataAll[col] = dataAll[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "#         le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "#         data_test_raw[str(col+'_numerical')]=le.transform(data_test_raw[col])\n",
    "print(dataCatAll.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataCatAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the labeled categorical data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "x_catAll_2 = mms.fit_transform(dataCatAll_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_catAll_2.shape)\n",
    "x_catAll_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore to x_trainDf and x_testDf - skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # hstack all the features and .\n",
    "# x_allDf = pd.concat([dataCatAll,dataFltAll],axis=1)\n",
    "# x_means = x_allDf.mean()\n",
    "# x_stds = x_allDf.std()\n",
    "# x_allDf = (x_allDf-x_means)/x_stds\n",
    "# x_allDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_Train = x_allDf.iloc[0:len(labelSs),:]\n",
    "# x_Test = x_allDf.iloc[len(labelSs):,:]\n",
    "# y_Train = labelSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot-encoding - skipped in order to keep the alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one-hot-encoding the categorical features\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "x_catAll = enc.fit_transform(dataCatAll)\n",
    "print(x_catAll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split x_train and x_test\n",
    "x_Train = sp.sparse.hstack((x_catAll[0:len(labelSs),:],x_catAll_2[0:len(labelSs),:],sp.sparse.csr_matrix(dataFltAll.as_matrix())[0:len(labelSs),:]))\n",
    "x_Test = sp.sparse.hstack((x_catAll[len(labelSs):,:],x_catAll_2[len(labelSs):,:],sp.sparse.csr_matrix(dataFltAll.as_matrix())[len(labelSs):,:]))\n",
    "y_Train = labelSs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the dimension of prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_Test.shape)\n",
    "print(x_Train.shape)\n",
    "print(y_Train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splite the training data for valication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "val_size = 0.2\n",
    "seed = 0\n",
    "x_train, x_val, y_train, y_val = cross_validation.train_test_split(x_Train, y_Train, test_size=val_size, random_state=seed)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x_Train\n",
    "del y_Train\n",
    "del x_catAll\n",
    "del dataCatAll\n",
    "# del trainDf\n",
    "# del testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_pred = []\n",
    "y_pred_val = []\n",
    "submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testList = ['XGBoostTrees', 'AdaBoosting', 'MLPRegressor','Ensemble']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeCV\n",
    "# # from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv = 3\n",
    "# # alphas = (1e-2,1e-1,1,1e1,1e2)\n",
    "# # alphas = (5,20,30,40)\n",
    "# alphas = [10]\n",
    "# regCV = RidgeCV(cv=cv,alphas = alphas)\n",
    "# regCV.fit(x_train,y_train)\n",
    "# print('alpha: {}\\n'.format(regCV.alpha_))\n",
    "# # print('cv_values_: {}\\n'.format(regCV.cv_values_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(regCV.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred_i = lossRestore(regCV.predict(x_Test),ymean,ystd)\n",
    "# y_pred.append(y_pred_i)\n",
    "# y_pred_val.append(lossRestore(regCV.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv = 3\n",
    "# # alphas = (1e-3,1e-2,1e-1,1,1e1,1e2,1e3)\n",
    "# # alphas = (0.0005,0.0007,0.001,0.003,0.005)\n",
    "# # alphas = [0.00005,0.0001,0.0003,0.0005]\n",
    "# alphas = [0.00005]\n",
    "# LassoCV = LassoCV(cv=cv,alphas = alphas)\n",
    "# LassoCV.fit(x_train,y_train)\n",
    "# print('alpha: {}\\n'.format(LassoCV.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(LassoCV.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred_i = lossRestore(LassoCV.predict(x_Test),ymean,ystd)\n",
    "# y_pred.append(y_pred_i)\n",
    "# y_pred_val.append(lossRestore(LassoCV.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipped - Random Forest - using mse rather than mae, because the mae implementation is much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# criterion = 'mse'\n",
    "# n_estimators = [30]\n",
    "# err = 999999999\n",
    "# n_estimator = 0\n",
    "# random_state = 0\n",
    "# for n_est in n_estimators:\n",
    "#     tmpRFReg = RandomForestRegressor(n_estimators = n_est,criterion = criterion, random_state = random_state)\n",
    "#     tmpRFReg.fit(x_train,y_train)\n",
    "#     err_i = mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(tmpRFReg.predict(x_val),ymean,ystd))\n",
    "#     print(err_i)\n",
    "#     if err_i < err:\n",
    "#         RFReg = tmpRFReg\n",
    "#         n_estimator = n_est\n",
    "#         err = err_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('n_estimator = {}'.format(n_estimator))\n",
    "# print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(RFReg.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred_i = lossRestore(RFReg.predict(x_Test),ymean,ystd)\n",
    "# y_pred.append(y_pred_i)\n",
    "# y_pred_val.append(lossRestore(RFReg.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_train_xgb = xgb.DMatrix(x_train.tocsc(),label=y_train)\n",
    "d_val_xgb = xgb.DMatrix(x_val.tocsc(),label = y_val)\n",
    "x_val_xgb = xgb.DMatrix(x_val.tocsc())\n",
    "d_test_xgb = xgb.DMatrix(x_Test.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_round = 2500\n",
    "# params = {'eval_metric':'mae','max_depth':6,'colsample_bytree':1,'alpha':1,'gamma':1,'eta':0.155,'min_child_weight':1}# 1134, 200 rounds\n",
    "# params = {'eval_metric':'mae','max_depth':6,'colsample_bytree':1,'alpha':1,'gamma':1,'eta':0.15,'min_child_weight':1}# 1133, 250 rounds\n",
    "params = {'eval_metric':'mae','max_depth':12,'colsample_bytree':1,'alpha':1,'gamma':0.01,'eta':0.05,'min_child_weight':1}# 1131, 700 rounds\n",
    "# params = {'eval_metric':'mae','max_depth':4,'colsample_bytree':1,'alpha':1,'gamma':0.01,'eta':0.05,'min_child_weight':1}# 1130, 2500 rounds\n",
    "\n",
    "watchlist  = [(d_val_xgb,'eval'), (d_train_xgb,'train')]\n",
    "gbt = xgb.train(params, d_train_xgb,num_round,watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(gbt.predict(x_val_xgb),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_i = lossRestore(gbt.predict(d_test_xgb),ymean,ystd)\n",
    "y_pred.append(y_pred_i)\n",
    "y_pred_val.append(lossRestore(gbt.predict(x_val_xgb),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_estimators = [30, 60, 90, 120]\n",
    "# n_estimators = [10,20,30,40]\n",
    "n_estimators = [6]\n",
    "base_estimator = Ridge(alpha = 40)\n",
    "err = 999999999\n",
    "n_estimator = 0\n",
    "random_state = 0\n",
    "for n_est in n_estimators:\n",
    "    tmpAdReg = AdaBoostRegressor(n_estimators = n_est, random_state = random_state,base_estimator = base_estimator)\n",
    "#     tmpAdReg = AdaBoostRegressor(n_estimators = n_est, random_state = random_state)\n",
    "    tmpAdReg.fit(x_train,y_train)\n",
    "    err_i = mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(tmpAdReg.predict(x_val),ymean,ystd))\n",
    "    print(err_i)\n",
    "    if err_i < err:\n",
    "        AdReg = tmpAdReg\n",
    "        n_estimator = n_est\n",
    "        err = err_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('n_estimator = {}'.format(n_estimator))\n",
    "print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(AdReg.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_i = lossRestore(AdReg.predict(x_Test),ymean,ystd)\n",
    "y_pred.append(y_pred_i)\n",
    "y_pred_val.append(lossRestore(AdReg.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipped - K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_neighbors = [5, 10, 15]\n",
    "# weights = 'distance'\n",
    "# err = 999999999\n",
    "# n_nns = 0\n",
    "# # random_state = 0\n",
    "# for n_nn in n_neighbors:\n",
    "#     tmpKNReg = KNeighborsRegressor(n_neighbors = n_nn,weights = weights)\n",
    "#     tmpKNReg.fit(x_train,y_train)\n",
    "#     err_i = mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(tmpKNReg.predict(x_val),ymean,ystd))\n",
    "#     print(err_i)\n",
    "#     if err_i < err:\n",
    "#         KNReg = tmpKNReg\n",
    "#         n_nns = n_nn\n",
    "#         err = err_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('n_neighbors = {}'.format(n_nns))\n",
    "# print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(KNReg.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_pred_i = lossRestore(KNReg.predict(x_Test),ymean,ystd)\n",
    "# y_pred.append(y_pred_i)\n",
    "# y_pred_val.append(lossRestore(KNReg.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # SVReg = svm.SVR(kernel = 'rbf')\n",
    "# # SVReg.fit(x_train,y_train)\n",
    "# SVReg = LinearSVR(C=0.5)\n",
    "# SVReg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(SVReg.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_pred_i = lossRestore(SVReg.predict(x_Test),ymean,ystd)\n",
    "# y_pred.append(y_pred_i)\n",
    "# y_pred_val.append(lossRestore(SVReg.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MLPReg = MLPRegressor(alpha = 1e-5, hidden_layer_sizes = (35,3),random_state=0,early_stopping=True)\n",
    "MLPReg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(lossRestore(y_val,ymean,ystd),lossRestore(MLPReg.predict(x_val),ymean,ystd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_i = lossRestore(MLPReg.predict(x_Test),ymean,ystd)\n",
    "y_pred.append(y_pred_i)\n",
    "y_pred_val.append(lossRestore(MLPReg.predict(x_val),ymean,ystd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save files for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ensemble the results\n",
    "y_pred.append(np.ndarray.mean(np.vstack(y_pred).T,axis=1))\n",
    "\n",
    "# ensembled y_val\n",
    "print('The loss of the ensembled result:')\n",
    "y_pred_val_en = np.ndarray.mean(np.vstack(y_pred_val).T,axis=1)\n",
    "mean_absolute_error(lossRestore(y_val,ymean,ystd),y_pred_val_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save all the predictions for submission\n",
    "for i,stri in enumerate(testList):\n",
    "    submission['id'] = subId\n",
    "    submission['loss']=pd.Series(data=y_pred[i])\n",
    "    submission.to_csv('../output/'+stri+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump([regCV, gbt, AdReg, MLPReg, y_pred, y_pred_val, x_train, x_val, y_train, y_val, x_Test]\n",
    "            ,'../output/models_data_on_raw_features.pkl',compress=3) \n",
    "\n",
    "# clf = joblib.load('filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
